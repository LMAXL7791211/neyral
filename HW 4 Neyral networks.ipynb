{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заглушка к 4 домашнему заданию курса "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в искусственные нейронные сети\n",
    "# Урок 4. Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробовать улучшить точность распознования образов cifar 10 сверточной нейронной сетью, рассмотренной на уроке. Приложить анализ с описанием того, что улучшает работу нейронной сети и что ухудшает.\n",
    "    </li>\n",
    "    <li>Описать также в анализе какие необоходимо внести изменения  в получившуюся у вас нейронную сеть если бы ей нужно было работать не с cifar10, а с MNIST, CIFAR100 и IMAGENET.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 1)           10        \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[3.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "\n",
    "# определение входных данных(8 массивов с 8 элементами)\n",
    "data = [[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n",
    "\n",
    "data = asarray(data)\n",
    "data = data.reshape(1, 8, 8, 1)\n",
    "\n",
    "# создание модели\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1)))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "\n",
    "# вывод описания созданной модели\n",
    "model.summary()\n",
    "\n",
    "# определение дектора вертикальной линии\n",
    "detector = [[[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]]]\n",
    "weights = [asarray(detector), asarray([0.0])]\n",
    "\n",
    "# сохранение весов в модель\n",
    "model.set_weights(weights)\n",
    "\n",
    "# применение фильтра к входным данным\n",
    "yhat = model.predict(data)\n",
    "\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть Lenet5.\n",
    "\n",
    "Lenet5 - это одна из первых сверточных нейронных сетей и она отражает характерные для сверточных нейронных сетей набор элементов - сверточные слои, пуллинг слои и полносвязные слои на конце нейронной сети. Данная архитектура послужила основой для многих современных архитектур сверточных нейронных сетей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 0.6774 - accuracy: 0.8240 - val_loss: 0.3395 - val_accuracy: 0.9092\n",
      "10000/10000 [==============================] - 2s 226us/step\n",
      "Test loss 0.3395, accuracy 90.92%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# загрузка тренировочных и тестовых данных\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# конвертация чисел из uint8 в float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# нормализация данных [0, 1]\n",
    "x_train /= 255 \n",
    "x_test /= 255 \n",
    "\n",
    "# трансформация лейблов в one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, 10) \n",
    "y_test = np_utils.to_categorical(y_test, 10) \n",
    "\n",
    "# изменение размерности массива в 4D массив\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28,1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import models, layers\n",
    "import keras\n",
    "\n",
    "# инициализация пустой модели\n",
    "model = Sequential()\n",
    "\n",
    "# первый сверточный слой\n",
    "model.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28,28,1), padding=\"same\"))\n",
    "\n",
    "# второй пуллинговый слой\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "\n",
    "# третий сверточный слой\n",
    "model.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "# четвертый пуллинговый слой\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "# пятый полносвязный слой\n",
    "model.add(layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "# сглаживание CNN выхода чтобы можно было его присоединить к полносвязногому слою\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# шестой полносвязный слой\n",
    "model.add(layers.Dense(84, activation='tanh'))\n",
    "\n",
    "# выходной слой с функцией активации softmax\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# компилияция модели\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "hist = model.fit(x=x_train,y=y_train, epochs=1, batch_size=128, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(test_score[0], test_score[1] * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример на Keras более сложной сверточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь попробуем сделать несколько усложненный вариант нейронной сети разобранной ранее. В ней будет на несколько слоев больше и в ней будет использоваться data augumentation, процедура позволяющая за счет искажений изображений увеличить количество тренировочных данных, а как мы знаем чем больше тренировочных данных тем лучше будет работать нейросеть. Для обучения нейросети будем использовать датасет cifar-10. В нем 10 категорий объектов, например - лошадь, лягушка, корабль. Данный датасет уже более сложен для нейронных сетей чем mnist, однако он намного проще датасетов наподобие imagenet где используются сотни классов и архитектуры нейронных сетей для подобных датасетов также понадобяться более сложные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 тренировочные примеры\n",
      "10000 тестовые примеры\n",
      "Использование data augmentation в реальном времени\n",
      "Epoch 1/1\n",
      "1563/1563 [==============================] - 71s 46ms/step - loss: 1.8735 - accuracy: 0.3106 - val_loss: 1.5553 - val_accuracy: 0.4267\n",
      "сохранить обученную модель как C:\\Users\\LMAXL\\Documents\\обучение\\14 основы нейронок\\4 вебинар материалы\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 4s 398us/step\n",
      "Test loss: 1.555319006729126\n",
      "Test accuracy: 0.42669999599456787\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras # расскоментируйте эту строку, чтобы начать обучение\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# разделение тренировочной и тестовой выборки\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'тренировочные примеры')\n",
    "print(x_test.shape[0], 'тестовые примеры')\n",
    "\n",
    "# преобразование матрицы чисел 0-9 в бинарную матрицу чисел 0-1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# компиляция модели\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False, \n",
    "        zca_epsilon=1e-06, \n",
    "        rotation_range=0, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0., \n",
    "        zoom_range=0., \n",
    "        channel_shift_range=0.,\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        rescale=None,\n",
    "        preprocessing_function=None,\n",
    "        data_format=None,\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # запуск data augmentation через fit\n",
    "    #datagen.fit(x_train)\n",
    "\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Использование data augmentation в реальном времени\n",
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.8427 - accuracy: 0.3175 - val_loss: 1.5456 - val_accuracy: 0.4442\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.5556 - accuracy: 0.4308 - val_loss: 1.4467 - val_accuracy: 0.4781\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.4430 - accuracy: 0.4772 - val_loss: 1.2805 - val_accuracy: 0.5435\n",
      "сохранить обученную модель как C:\\Users\\LMAXL\\Documents\\обучение\\14 основы нейронок\\4 вебинар материалы\\saved_models\\keras_cifar10_trained_model2.h5 \n",
      "10000/10000 [==============================] - 4s 402us/step\n",
      "Test loss: 1.280473300552368\n",
      "Test accuracy: 0.5435000061988831\n"
     ]
    }
   ],
   "source": [
    "# Увеличим количество эпох до 3\n",
    "\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model2.h5'\n",
    "\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model2 = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv2D(32, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(num_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# компиляция модели\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model2.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model2.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение количества эпох предсказуемо повысило качество (с 0.42 до 0.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не используется data augmentation\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 1.7986 - accuracy: 0.3412 - val_loss: 1.5177 - val_accuracy: 0.4572\n",
      "сохранить обученную модель как C:\\Users\\LMAXL\\Documents\\обучение\\14 основы нейронок\\4 вебинар материалы\\saved_models\\keras_cifar10_trained_model3.h5 \n",
      "10000/10000 [==============================] - 4s 389us/step\n",
      "Test loss: 1.5176774404525757\n",
      "Test accuracy: 0.45719999074935913\n"
     ]
    }
   ],
   "source": [
    "# уберем аугментацию\n",
    "\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "data_augmentation = False\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model3.h5'\n",
    "\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model3 = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model3.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Conv2D(32, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(512))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(num_classes))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# компиляция модели\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model3.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "\n",
    "\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model3.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model3.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model3.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без аугментации качество немного выросло (с 0.42 до 0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Использование data augmentation в реальном времени\n",
      "Epoch 1/1\n",
      "1563/1563 [==============================] - 186s 119ms/step - loss: 1.8171 - accuracy: 0.3288 - val_loss: 1.4913 - val_accuracy: 0.4567\n",
      "сохранить обученную модель как C:\\Users\\LMAXL\\Documents\\обучение\\14 основы нейронок\\4 вебинар материалы\\saved_models\\keras_cifar10_trained_model4.h5 \n",
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "Test loss: 1.491331782913208\n",
      "Test accuracy: 0.45669999718666077\n"
     ]
    }
   ],
   "source": [
    "# увеличим колво фильтров в 1х слоях до 128: \n",
    "\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model4.h5'\n",
    "\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model4 = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model4.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(128, (3, 3)))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(64, (3, 3)))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(512))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(num_classes))\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# компиляция модели\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model4.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model4.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model4.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model4.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение фильтров до 128 также немного улучшило качество (с 0.42 до 0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Использование data augmentation в реальном времени\n",
      "Epoch 1/1\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.6406 - accuracy: 0.3950 - val_loss: 1.2379 - val_accuracy: 0.5484\n",
      "сохранить обученную модель как C:\\Users\\LMAXL\\Documents\\обучение\\14 основы нейронок\\4 вебинар материалы\\saved_models\\keras_cifar10_trained_model5.h5 \n",
      "10000/10000 [==============================] - 4s 389us/step\n",
      "Test loss: 1.237885891342163\n",
      "Test accuracy: 0.5483999848365784\n"
     ]
    }
   ],
   "source": [
    "# заменим optimizer на adam: \n",
    "\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model5.h5'\n",
    "\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model5 = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model5.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Conv2D(32, (3, 3)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Conv2D(64, (3, 3)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(512))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes))\n",
    "model5.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "opt = 'adam' #keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# компиляция модели\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model5.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model5.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model5.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model5.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "смена Optimizer на adam существенно улучшило качество (с 0.42 до 0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не используется data augmentation\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 190s 4ms/step - loss: 1.5657 - accuracy: 0.4274 - val_loss: 1.2388 - val_accuracy: 0.5543\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 189s 4ms/step - loss: 1.1066 - accuracy: 0.6070 - val_loss: 0.9481 - val_accuracy: 0.6702\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 189s 4ms/step - loss: 0.9436 - accuracy: 0.6687 - val_loss: 0.8293 - val_accuracy: 0.7157\n",
      "сохранить обученную модель как C:\\Users\\LMAXL\\Documents\\обучение\\14 основы нейронок\\4 вебинар материалы\\saved_models\\keras_cifar10_trained_model6.h5 \n",
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "Test loss: 0.8292902936935425\n",
      "Test accuracy: 0.7156999707221985\n"
     ]
    }
   ],
   "source": [
    "# применим все вышеперечисленное: \n",
    "\n",
    "\n",
    "# установка параметров нейросети\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "data_augmentation = False\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model6.h5'\n",
    "\n",
    "\n",
    "# конфигурирование слоев нейросети\n",
    "model6 = Sequential()\n",
    "\n",
    "# слои нейросети отвественные за свертку и max-pooling\n",
    "model6.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Conv2D(128, (3, 3)))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model6.add(Dropout(0.25))\n",
    "\n",
    "model6.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Conv2D(64, (3, 3)))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model6.add(Dropout(0.25))\n",
    "\n",
    "# полносвязные слои нейронной сети\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(512))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "model6.add(Dense(num_classes))\n",
    "model6.add(Activation('softmax'))\n",
    "\n",
    "# инициализация RMSprop optimizer\n",
    "opt = 'adam' #keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# компиляция модели\n",
    "model6.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Не используется data augmentation')\n",
    "    model6.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Использование data augmentation в реальном времени')\n",
    "    # Препроцессинг и data augmentation в реальном времени:\n",
    "#\n",
    "    # запуск data augmentation через fit_generator\n",
    "    model6.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# сохранение модели и весов\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model6.save(model_path)\n",
    "print('сохранить обученную модель как %s ' % model_path)\n",
    "\n",
    "# проверка работы обученной модели\n",
    "scores = model6.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "    все изменения улучшили качество:\n",
    "        1. Увеличение колва эпох до 3\n",
    "        2. Отключение аугментации\n",
    "        3. Увеличение колва фильтров в 2х первых сверточных слоях до 128\n",
    "        4. Замена optimizer на adam \n",
    "        \n",
    "        В итоговой сети accuracy выросло с 0.42 до 0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Для того чтобы работать с MNIST, CIFAR100, IMAGENET - нужно изменить параметры (размерность) входных данных (под размеры изображений в датасете), а также откорректировать значение num_classes под количество классов выбранного датасета.\n",
    "\n",
    "Также, при работе с бОльшими и более сложными датасетами потребуется усложнить модель (увеличив колво слоев)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
